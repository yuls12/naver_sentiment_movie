{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "sensitive-provider",
   "metadata": {},
   "source": [
    "### scikit-learn으로 NSMC 감정분석 하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "boring-elevation",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_documents(filename):\n",
    "    with open(filename, encoding='utf-8') as f:\n",
    "        documents = [line.split('\\t') for line in f.read().splitlines()]\n",
    "        documents = documents[1:]\n",
    "        \n",
    "    return documents\n",
    "    \n",
    "train_docs = read_documents(\"data/nsmc/ratings_train.txt\")\n",
    "test_docs = read_documents(\"data/nsmc/ratings_test.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "forward-darkness",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(train_docs))\n",
    "print(len(test_docs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "measured-ethics",
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_cleaning(doc):\n",
    "    # 한국어를 제외한 글자를 제거하는 함수.\n",
    "    \n",
    "    doc = re.sub(\"[^ㄱ-ㅎㅏ-ㅣ가-힣 ]\", \"\", doc)\n",
    "        \n",
    "    return doc\n",
    "\n",
    "def define_stopwords(path):\n",
    "    \n",
    "    SW = set()\n",
    "    # 불용어를 추가하는 방법 1.\n",
    "    # SW.add(\"있다\")\n",
    "    \n",
    "    # 불용어를 추가하는 방법 2.\n",
    "    # stopwords-ko.txt에 직접 추가\n",
    "    \n",
    "    with open(path) as f:\n",
    "        for word in f:\n",
    "            SW.add(word)\n",
    "            \n",
    "    return SW\n",
    "\n",
    "def text_tokenizing(doc):\n",
    "    return [word for word in mecab.morphs(doc) if word not in SW and len(word) > 1]\n",
    "    \n",
    "    # wordcloud를 위해 명사만 추출하는 경우.\n",
    "    #return [word for word in mecab.nouns(doc) if word not in SW and len(word) > 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "complimentary-refrigerator",
   "metadata": {},
   "source": [
    "#### 데이터를 품사 태그를 붙여서 토크나이징"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "explicit-romania",
   "metadata": {},
   "outputs": [],
   "source": [
    "from konlpy.tag import Mecab\n",
    "from konlpy.tag import Okt\n",
    "import json\n",
    "import os\n",
    "import re\n",
    "from pprint import pprint\n",
    "\n",
    "okt = Okt()\n",
    "mecab = Mecab()\n",
    "\n",
    "SW = define_stopwords(\"data/stopwords-ko.txt\")\n",
    "\n",
    "if os.path.exists('train_docs.json'):\n",
    "    with open(\"train_docs.json\", encoding='utf-8') as f:\n",
    "        train_data = json.load(f)\n",
    "else:\n",
    "    train_data = [(text_tokenizing(text_cleaning(line[1])), line[2]) for line in train_docs if text_tokenizing(line[1])]\n",
    "    #train_data = [(text_tokenizing(line[1]), line[2]) for line in train_docs if text_tokenizing(line[1])]\n",
    "    \n",
    "    with open(\"train_docs.json\", 'w', encoding='utf-8') as f:\n",
    "        json.dump(train_data, f, ensure_ascii=False, indent='\\t')\n",
    "        \n",
    "if os.path.exists('test_docs.json'):\n",
    "    with open(\"test_docs.json\", encoding='utf-8') as f:\n",
    "        test_data = json.load(f)\n",
    "else:\n",
    "    test_data = [(text_tokenizing(text_cleaning(line[1])), line[2]) for line in test_docs if text_tokenizing(line[1])]\n",
    "    #test_data = [(text_tokenizing(line[1]), line[2]) for line in test_docs if text_tokenizing(line[1])]\n",
    "    with open(\"test_docs.json\", 'w', encoding='utf-8') as f:\n",
    "        json.dump(test_data, f, ensure_ascii=False, indent='\\t')\n",
    "\n",
    "print(train_data[0])\n",
    "print(test_data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "junior-pharmacology",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_data[:3])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "alive-declaration",
   "metadata": {},
   "source": [
    "#### nltk 라이브러리를 이용하여 전처리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "accredited-designation",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "\n",
    "total_tokens = [token for doc in train_data for token in doc[0]]\n",
    "print(len(total_tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "posted-bones",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = nltk.Text(total_tokens, name='NMSC')\n",
    "print(len(set(text.tokens)))\n",
    "pprint(text.vocab().most_common(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "wrong-subdivision",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import platform\n",
    "from matplotlib import font_manager, rc\n",
    "%matplotlib inline\n",
    "\n",
    "path = \"c:/Windows/Fonts/malgun.ttf\"\n",
    "if platform.system() == 'Darwin':\n",
    "    rc('font', family='AppleGothic')\n",
    "elif platform.system() == 'Windows':\n",
    "    font_name = font_manager.FontProperties(fname=path).get_name()\n",
    "    rc('font', family=font_name)\n",
    "else:\n",
    "    print('Unknown system... sorry~~~~')\n",
    "\n",
    "plt.figure(figsize=(16, 10))\n",
    "text.plot(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cleared-dependence",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 여러 리스트들을 하나로 묶어 주는 함수\n",
    "def list_to_str(List): \n",
    "    return \" \".join(List)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "respiratory-reach",
   "metadata": {},
   "source": [
    "### Linear Classifier와 Support Vector Machine으로 nsmc 분류하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "forbidden-bacon",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "import numpy as np\n",
    "\n",
    "train_x = [list_to_str(doc) for doc, _ in train_data]\n",
    "test_x = [list_to_str(doc) for doc, _ in test_data]\n",
    "train_y = [label for _, label in train_data]\n",
    "test_y = [label for _, label in test_data]\n",
    "\n",
    "#print(len(train_x), len(train_y))\n",
    "print(\"For %d train data\" % len(train_x))\n",
    "#print(len(test_x), len(test_y))\n",
    "\n",
    "learner = Pipeline([\n",
    "    ('vect', CountVectorizer()),\n",
    "    ('clf', SGDClassifier(loss='perceptron', penalty='l2',\n",
    "                         alpha=1e-4, random_state=42,\n",
    "                         max_iter=100))\n",
    "])\n",
    "\n",
    "learner2 = Pipeline([\n",
    "    ('vect', CountVectorizer()),\n",
    "    ('clf', SVC(kernel='linear'))\n",
    "    \n",
    "])\n",
    "\n",
    "learner3 = Pipeline([\n",
    "    ('vect', CountVectorizer()),\n",
    "    ('clf', SVC(kernel='poly', degree=8))\n",
    "])\n",
    "\n",
    "learner4 = Pipeline([\n",
    "    ('vect', CountVectorizer()),\n",
    "    ('clf', SVC(kernel='rbf'))\n",
    "])\n",
    "\n",
    "learner5 = Pipeline([\n",
    "    ('vect', CountVectorizer()),\n",
    "    ('clf', SVC(kernel='sigmoid'))\n",
    "])\n",
    "\n",
    "learner6 = Pipeline([\n",
    "    ('vect', CountVectorizer()),\n",
    "    ('mb', MultinomialNB())\n",
    "])\n",
    "\n",
    "classifier = learner2\n",
    "\n",
    "\n",
    "classifier.fit(train_x, train_y)\n",
    "train_predict = classifier.predict(train_x)\n",
    "train_accuracy = np.mean(train_predict == train_y)\n",
    "\n",
    "test_predict = classifier.predict(test_x)\n",
    "test_accuracy = np.mean(test_predict == test_y)\n",
    "\n",
    "\n",
    "print(\"For %d test data\" % len(test_x))\n",
    "\n",
    "print(\"Training Accuracy : %.2f\" % train_accuracy)\n",
    "print(\"Test Accuracy : %.2f\" % test_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "original-today",
   "metadata": {},
   "source": [
    "### < 실험결과 >\n",
    "\n",
    "#### Linear Classifier 학습하여, test accuracy를 측정. (learner)\n",
    "\n",
    "1. 명사만 추출 : 0.51\n",
    "2. 전처리 하지 않고 형태소 분석 : 0.67\n",
    "3. 전처리 하고 형태소 분석 : 0.71\n",
    "\n",
    "-----여기까진 top 500 features만 사용--------\n",
    "4. 3 + 모든 feature : 0.76"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "empirical-birmingham",
   "metadata": {},
   "source": [
    "### < 실험결과 >\n",
    "\n",
    "#### SVM Classifier 학습하여, test accuracy를 측정. (learner2)\n",
    "\n",
    "1. 명사만 추출 : 0.53\n",
    "2. 전처리 하지 않고 형태소 분석 : 0.72\n",
    "3. 전처리 하고 형태소 분석 : 0.77\n",
    "\n",
    "-----여기까진 top 500 features만 사용--------\n",
    "4. 3 + 모든 feature : 0.81"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "interesting-final",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
